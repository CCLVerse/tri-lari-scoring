---
title: "`r params$title`"
output: html_document
params: 
  title: NULL
  data: NULL
  factor_name: ""
  regex_pattern: TRUE
  raters: NULL
  sample_name: ""
  show_model: FALSE
  rowmeans_columns_list: NULL
  columns_for_analyses: NULL
  competency_cols_names: ""
  columns_to_remove_missing_values: NULL
  factor_weights_competency_cols: NULL
  factor_weights_cols: NULL
  competency_cols_with_factor_weights: NULL
  outcome_raters: NULL
  outcome_colname: NULL
---



```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```



```{css, echo=FALSE}
/* Include the Google Fonts stylesheet */
@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap');

/* Apply the font to the body */
body {
  font-family: 'Roboto', sans-serif;
}

th:not(:first-child), td:not(:first-child){
  text-align: center !important;
}


```



```{r subtitle, results='asis', echo=FALSE}

cat(sprintf("#### <span style='color:#9e9e9e;'>%s Sample</span>", stringi::stri_trans_totitle(sample_name)))

```


```{r assign-values, echo=FALSE, warning=FALSE}

df <- params$data
competency_cols_names <- params$competency_cols_names
factor_name <- params$factor_name
raters <- params$raters
regex_pattern <- params$regex_pattern
sample_name <- params$sample_name
show_model <- params$show_model
rowmeans_columns_list <- params$rowmeans_columns_list
columns_for_analyses <- params$columns_for_analyses
columns_to_remove_missing_values <- params$columns_to_remove_missing_values
factor_weights_competency_cols <- params$factor_weights_competency_cols
factor_weights_cols <- params$factor_weights_cols
competency_cols_with_factor_weights <- params$competency_cols_with_factor_weights
outcome_raters <- params$outcome_raters
outcome_colname <- params$outcome_colname


competency_full_name <- get_competency_full_name(factor_name)


```


### Lavaan TRI<span style='color:gold;'>/</span>LARI Analyses


```{r basic-missing-statistic, echo=FALSE, cache=FALSE, warning=FALSE, include=FALSE}

cat("#### Checking for missing data before building the models\n\n")
cat("### Missing Data Counts\n\n")

missing_data <- count_missing_values(df=df)

missing_data %>%
  DT::datatable(filter = 'top', 
                style = "bootstrap4",
                class = "table table-striped table-bordered",
                options = list(paging = TRUE, 
                               pageLength = 10, 
                               autoWidth = TRUE),
                caption = htmltools::tags$caption(
                  style = 'caption-side: bottom; text-align: center;',
                  'Table 2: ', htmltools::em('Missing data count.'))) %>%
  DT::formatStyle(
    'Percentage',
    color = DT::styleInterval(0.70, c('green', 'red')),
  ) %>% 
  DT::formatPercentage(2) 


```




```{r first-order, results="asis", echo=FALSE, warning=FALSE}

if(tolower(sample_name) == 'normative') {
  cat("---\n\n")
  cat("#### Models to Build\n\n")
  cat("There are two models to build:\n\n")
  cat("1. First Order Model\n\n")
  cat("2. Bifactor Model\n\n")

  cat("---\n\n")
  
  cat("#### Required Values\n")

  cat("Both models require the following values:\n\n")

  cat("- **competency_cols_names:** Column names that make up the competency or the regex pattern to identify the columns. When providing a regex pattern, set `regex_pattern` to TRUE.\n\n") 
  cat('*Example: "BMK_S1_34|BMK_S1_73|BMK_S1_77"*\n\n')
  cat("- **factor_name:** Name of the competency. This name is added in the `lavaan` analysis results.\n\n")
  cat("- **regex_pattern:** Indicate whether the `competency_cols` value is a regex pattern or column names.\n\n")

  cat("- **raters:** All the raters required in the analyses.\n\n")
  
  cat("---\n\n")
  cat("### First order model\n\n")

  first_level <- build_first_level_model(df=df
                                       , competency_cols_names=competency_cols_names
                                       , factor_name=factor_name
                                       , raters=raters
                                       , regex_pattern = regex_pattern)

  if(show_model){
    
    cat(first_level %>% 
      kableExtra::kable(col.names = "", caption = "Displaying `Lavaan` first order model used in analyses") %>%
      kableExtra::kable_styling()) 
  }
  
  
  fits <- lavaan_model_fit(df=df, model=first_level)
} 

```



```{r bifactor, results="asis", warning=FALSE, echo=FALSE}

if(tolower(sample_name) == 'normative'){ 
  cat("### Bifactor model\n") 

  bifactor <- build_bifactor_model(df=df
                                   , competency_cols_names=competency_cols_names
                                   , factor_name=factor_name
                                   , raters=raters
                                   , regex_pattern = regex_pattern)
  
  if(show_model){
    cat(bifactor %>% 
      kableExtra::kable(col.names = "", caption = "Displaying `Lavaan` Bifactor model used in analyses") %>%
      kableExtra::kable_styling())
  }
  
  bifactor_fits <- lavaan_model_fit(df=df, model=bifactor)
  
} else {
  cat("The lavaan first order and bifactor models are run only on the normative sample. Ideally, the factor weights from the normative samples generalize to new data samples with similar statistics as the normative samples. To test the generalizability, we use the same factor weights from normative sample on the cross-validation sample. Therefore, the first order and bifactor models are not run for cross-validation sample. ")
}

```



```{r factor-weights, echo=FALSE, warning=FALSE}

## This will run for normative samples only. 

## The condition below will get the factor scoring weights from the model fitted values. The result is a single-row dataframe of factor_weights that can be merged with the raw data frame. The factor weights will be stored in global variables for cross-validation sample to use. 

if(tolower(sample_name) == 'normative'){
  
  factor_weights <<- get_factor_scoring_weights(fitted_values=bifactor_fits$orthogonal$fitted
                                                , which="fsm")
}

```



```{r remove-missing-values, echo=FALSE, warning=FALSE}

## remove missing values for the columns

df <- df %>% 
  select(matches(columns_for_analyses)) %>% 
  remove_missing_from_competency(df=.
                                , column_names=columns_to_remove_missing_values
                                , regex_pattern = TRUE)


```




```{r merge-factor-weights, echo=FALSE, warning=FALSE}

## merge the factor weights with the raw item responses. This will broadcast for all rows
df <- df %>% 
      dplyr::bind_cols(factor_weights) %>%
      as.data.frame()


## scale and the factor weights
df_merged <- scale_and_center_columns(df=df
                              , column_names = colnames(factor_weights)
                              , center = FALSE)

## remove df for memory maangement
remove(df)

```




```{r factor-score, echo=FALSE, results='asis', warning=FALSE}

## to calculate the factor scores, we need to first create models for lavaan. The code below generates the model for all raters, reputation and arena. It creates factor scores by weighting/summing the item responses by the relevant factor scoring weights

factor_score_equations <- generate_factor_scores(df=df_merged
                      , competency_cols = factor_weights_competency_cols
                      , factor_weights_cols = factor_weights_cols
                      , factor_name = factor_name
                      , latent_variables=c(raters, "reputation", "arena")
                      , regex_pattern = TRUE
                      , suffix = "LV")

# if(show_model){
#     cat(factor_score_equations %>% 
#       kableExtra::kable(col.names = "", caption = "Equations to calculate factor scores") %>%
#       kableExtra::kable_styling())
# }


df_merged <- calculate_factor_scores(df=df_merged
                                     , factor_score_equations=factor_score_equations)


n = nrow(df_merged)

```




```{r competency-statistics, echo=FALSE, warning=FALSE}

## calculate the statistics (mean and sd) of the data. This data will be merged with the factor_weights statistics to display in table 1. 

statistics <- compute_statistics(df=df_merged
                                 , columns = competency_cols_with_factor_weights
                                 , regex_pattern = TRUE)


```

---

### Table 1

```{r statistics-table, echo=FALSE, warning=FALSE}

t_factor_weights <- t(factor_weights) %>% as.data.frame() %>% rownames_to_column(var="Item")

# Extract trait/arena names
trait_names <- unique(gsub(sprintf(".*_mean_(\\w+)_%s_.*", factor_name), "\\1", t_factor_weights$Item))

# Merge data using dplyr and tidyr
result_df <- t_factor_weights %>%
  separate(Item, into = c("Trait", "Factor"), sep = paste0("_", factor_name, "_")) %>%
  pivot_wider(names_from = "Factor", values_from = "V1") %>%
  left_join(statistics, by = "Trait") %>%
  mutate(across(where(is.numeric), ~ formatC(., format="f", digits = 3))) %>%
  mutate(`Mean (SD)` = sprintf("%s (%s)", Mean, SD)) %>%
  select(Item = Trait, `Mean (SD)`, Self, Direct_Report, Boss, Peer, reputation, arena)
  
colnames(result_df) <- c("Item", "Mean (SD)", "Self", "Direct Report", "Boss", "Peer", "Reputation", "Trait/Arena")


result_df %>%
  kableExtra::kable(caption=sprintf('Table 1: Item Means, Standard Deviations, and Factor Scoring Weights Derived from the TRI/LARI Model (**%s Sample**)',sample_name)) %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "responsive")) %>%
  kableExtra::add_header_above(c(" " = 2, "Factor Scoring Weights" = 6)) %>%
  kableExtra::footnote(general = sprintf("**N = %s**", format(n, big.mark=",")))


```




```{r conventional-average, echo=FALSE, warning=FALSE}

## As a way to compute the correlation of LV with the conventional averaged values of the scores, we'll calculate conventional average of scores by each rater type.  

df_merged <- calculate_rowmeans(df=df_merged
                 , new_cols=c("Self_Avg", "Direct_Report_Avg", "Boss_Avg", 
                              "Peer_Avg", "Inf_Avg", "All_Avg")
                 , mean_cols=rowmeans_columns_list)


```



```{r performance-item, echo=FALSE, warning=FALSE}

## calculate performance scores. The performance scores are in section 5 of the BMK. 

outcome_rater_cols <- get_column_names(df=df_merged, pattern=outcome_raters)

## Create new var for performance item 1 containing the average across the three rater groups.
df_merged <- aggregate_competency_cols(df=df_merged
                                    , columns=outcome_rater_cols
                                    , aggregated_column_name=outcome_colname)


```

---

### Table 2

#### 2a. TRI<span style="color:gold;">/</span>LARI-Based Scores
```{r correlation, echo=FALSE, warning=FALSE}

## generates a correlation table. 

## select columns that contains _LV or _all_rater in the name. LV are all factor scores from the sample. The _all_rater is to capture the aggregated performance column. 

lv_correlation <-  compute_correlation(df=df_merged
                                   , x = ".*_all_rater|LV"
                                   , regex_pattern = TRUE)

diag(lv_correlation) <- "◉"
rownames(lv_correlation) <- c("Self", "Direct Report", "Peer", "Boss", "Reputation", "Trait/Arena", "Performance")
colnames(lv_correlation) <- c("Self", "Direct Report", "Peer", "Boss", "Reputation", "Trait/Arena", "Performance")


if(sample_name == "Normative"){
  lv_correlation[lower.tri(lv_correlation)] <- "◉"
} else {
  lv_correlation[upper.tri(lv_correlation)] <- "◉"
}

lv_correlation %>% 
  kableExtra::kable(caption = sprintf("Correlations Among <span style='color:#00838f;'>**%s**</span> Scores: TRI/LARI-based Scores and Avergaed Scores", competency_full_name)) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::column_spec(1, bold=TRUE)

```


#### 2b. Averaged Scores

```{r tri-lari-avg-correlation, echo=FALSE, warning=FALSE}

## similar to above, but instead of LV scores, we'll extract conventional average values. 

avg_perf_correlation <- compute_correlation(df=df_merged
                                   , x = ".*_Avg|all_rater"
                                   , regex_pattern = TRUE
                                   )

avg_perf_correlation_mat <- avg_perf_correlation %>%
  rownames_to_column(var = "Var") %>%
  select(Var, outcome_colname)

diag(avg_perf_correlation) <- "◉"
rownames(avg_perf_correlation) <- c("Self", "Direct Report", "Peer", "Boss", "All Informants", "All Raters", "Performance")
colnames(avg_perf_correlation) <- c("Self", "Direct Report", "Peer", "Boss", "All Informants", "All Raters", "Performance")

if(sample_name == "Normative"){
  avg_perf_correlation[lower.tri(avg_perf_correlation)] <- "◉"
} else {
  avg_perf_correlation[upper.tri(avg_perf_correlation)] <- "◉"
}

avg_perf_correlation %>% 
  kableExtra::kable(caption = sprintf("Correlations Among <span style='color:#00838f;'>**%s**</span> Scores: TRI/LARI-based Scores and Avergaed Scores", competency_full_name)) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::column_spec(1, bold=TRUE) %>% 
  kableExtra::footnote(sprintf("Correlations above the diagonal in each matrix are from the Normative sample, those below the diagonal are from the cross-validation sample.**$N_{%s}$ = %s**. Correlations ≥ |.03| are significant at p < .05. ", tolower(sample_name), format(n, big.mark=",")))


```


---

### Table 3

```{r compute-norm-correlation, results='asis', echo=FALSE, warning=FALSE}


## Generate header based on the sample name
if(tolower(sample_name) == 'normative'){
  cat("#### 3a. Normative Sample\n")
} else {
  cat("#### 3a. Normative Sample\n")
  cat(sprintf("Correlation matrix for normative sample is in another file. If all files are stored in the same location, you can view it by clicking [here](normative_%s.html#3a-normative-sample)\n\n",factor_name))

  cat("#### 3b. Cross-validation Sample\n")
}


## this will perform the correlation of LV and conventional scores including performance. 

lv_x = get_column_names(df=df_merged, pattern = ".*_LV")
avg_y = get_column_names(df=df_merged, pattern = ".*_(Avg|all_rater)")
avg_y <- df_merged[avg_y]

correlation <- compute_correlation(df=df_merged
                             , x = lv_x
                             , y = avg_y
                             )

perf_correlation <- correlation %>% 
  rownames_to_column(var = "Var") %>%
  select(Var, outcome_colname)

rownames(correlation) <- c("Self", "Direct Report", "Peer", "Boss", "Reputation", "Trait/Arena")
colnames(correlation) <- c("Self", "Direct Report", "Boss", "Peer", "All Informants", "All Raters", "Performance")

correlation %>%
  kableExtra::kable(caption = "Correlations Among TRI/LARI-based Scores, Averaged Scores, and Performance Criterion.") %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::column_spec(1, bold=TRUE) %>%
  kableExtra::add_header_above(c(" " = 1, "Averaged Scores" = 7))


## Generate header based on the sample name
if(tolower(sample_name) == 'normative'){
  cat("#### 3b. Cross-validation Sample\n")
  cat(sprintf("Correlation matrix for Cross-validation sample is in another file. If all files are stored in the same location, you can view it by clicking [here](cross-validation_%s.html#3b-cross-validation-sample)\n\n",factor_name))
} 


```



```{r save-df, echo=FALSE}

if(tolower(sample_name) == 'normative'){

  rn <- 20
  set.seed(1234)
  
  selective_colnames <- c("ESI_Key", "Self_LV", "Direct_Report_LV", "Peer_LV", "Boss_LV", "reputation_LV", "arena_LV", "Self_Avg", "Direct_Report_Avg", "Boss_Avg", "Peer_Avg", "Inf_Avg", "All_Avg", "BMK_S5_1_mean_all_rater")
  
  competency_name <- rep(factor_name, rn)
  
  # Randomly select 20 rows from the dataframe
  selected_rows <- sample(nrow(df_merged), rn, replace = FALSE)
  
  # Create a new dataframe with the selected rows
  random_sample_df <- df_merged[selected_rows, ] %>% 
    dplyr::select(selective_colnames) %>%
    tibble::add_column(competency = competency_name, .before="Self_LV")
  
  if(!exists("factors_df")){
    factors_df <- data.frame()
  }
  
  factors_df <<- dplyr::bind_rows(factors_df, random_sample_df)
}

```


---

### Linear Models

#### Model 1: `r sample_name` Sample

```{r linear-model1, results="asis", echo=FALSE, warning=FALSE}

## Fit linear model for LV

lv_cols <- get_column_names(df_merged, ".*_LV") %>% paste0(., collapse = " + ")
formula <- as.formula(sprintf("%s ~ %s", outcome_colname, lv_cols))


mod1 <- linear_model(formula=formula, df=df_merged)

summary1 <- mod1$summary

coefs1 <- summary1$coefficients %>% 
  as.data.frame() %>%
  mutate(across(where(is.numeric), ~ formatC(., format="f", digits=2))) %>%
  rownames_to_column(var = "Var") 

std1 <- coefs1 %>% 
  mutate(
    Standardized = case_when(
    `Pr(>|t|)` > 0.05 ~ sprintf("$%s^{NS}$",Standardized),
    TRUE ~ Standardized
  )) %>% 
  select(Var, Standardized) %>%
  filter(Var != "(Intercept)")


## extract standardized weights from the coefficient table. 

std1 <- coefs1 %>% 
  mutate(Standardized = as.numeric(Standardized),  # Convert to numeric
         Standardized = case_when(
           `Pr(>|t|)` > 0.05 ~ paste0(formatC(Standardized, format="f", digits=2), "*"), 
           TRUE ~ formatC(Standardized, format="f", digits=2)
         )) %>% 
  select(Var, Standardized) %>%
  filter(Var != "(Intercept)")

```




```{r linear-model2, results="asis", echo=FALSE, warning=FALSE}

## Fit linear model for conventional average score columns

pattern = sprintf(".*(%s)_Avg", paste0(stringr::str_replace_all(raters, " ", "_"), collapse = "|"))

avg_cols <- get_column_names(df_merged, pattern) %>% paste0(., collapse = " + ")
formula <- as.formula(sprintf("%s ~ %s", outcome_colname, avg_cols))

mod2 <- linear_model(formula=formula, df=df_merged)

summary2 <- mod2$summary

coefs2 <- summary2$coefficients %>% 
  as.data.frame() %>%
  mutate(across(where(is.numeric), ~ formatC(., format="f", digits=2))) %>%
  rownames_to_column(var = "Var")


## extract standardized weights from the coefficient table. 

std2 <- coefs2 %>%   
  mutate(Standardized = case_when(
    `Pr(>|t|)` > 0.05 ~ sprintf("$%s^{NS}$",Standardized),
    TRUE ~ Standardized
  )) %>% 
  select(Var, Standardized) %>%
  filter(Var != "(Intercept)")

```



```{r linear-model3, results="asis", echo=FALSE, warning=FALSE}

## Fit linear model for LV and conventional average score columns

lv_avg_cols <- paste0(c(avg_cols, lv_cols), collapse=" + ")
formula <- as.formula(sprintf("%s ~ %s", outcome_colname, lv_avg_cols))

mod3 <- linear_model(formula=formula, df=df_merged)

summary3 <- mod3$summary

coefs3 <- summary3$coefficients %>% 
  as.data.frame() %>%
  mutate(across(where(is.numeric), ~ formatC(., format="f", digits=2))) %>%
  rownames_to_column(var = "Var")

## extract standardized weights from the coefficient table. 

std3 <- coefs3 %>% 
    mutate(Standardized = case_when(
    `Pr(>|t|)` > 0.05 ~ sprintf("$%s^{NS}$",Standardized), 
    TRUE ~ Standardized
  )) %>% 
  select(Var, Standardized) %>%
  filter(Var != "(Intercept)")

```



```{r reg-weights, echo=FALSE, warning=FALSE}

## extract r-squared from each model to add to the table. The table consists of standardized weights for all 3 models. 

r2 <- data.frame(Var = "$R^{2}$", 
                `1` = summary1$r.squared, 
                `2` = summary2$r.squared, 
                `3` = summary3$r.squared) %>%
  rename(`1` = X1, `2` = X2, `3` = X3) %>% 
  mutate(across(where(is.numeric), ~ formatC(., format="f", digits=2)))

reg_weights_lv <- perf_correlation %>% 
  left_join(std1, by = "Var") %>%
  left_join(std2, by="Var") %>% 
  left_join(std3, by="Var") %>% 
  rename(
    `r with perf` = outcome_colname,
    `1` = Standardized.x, 
    `2` = Standardized.y, 
    `3` = Standardized) 


reg_weights_avg <- avg_perf_correlation_mat %>% 
  left_join(std1, by = "Var") %>%
  left_join(std2,by = "Var") %>%
  left_join(std3, by = "Var") %>%
  rename(
    `r with perf` = outcome_colname,
    `1` = Standardized.x, 
    `2` = Standardized.y, 
    `3` = Standardized) %>%
  filter(Var != outcome_colname) %>%
  bind_rows(r2)

reg_weights_table <- reg_weights_lv %>%
  bind_rows(reg_weights_avg) %>%
  mutate(Var = case_when(
    str_detect(Var, regex("Self_(LV|Avg)", ignore_case=TRUE)) ~ "Self", 
    str_detect(Var, regex("Direct_Report_(LV|Avg)", ignore_case=TRUE)) ~ "Direct Report", 
    str_detect(Var, regex("Peer_(LV|Avg)", ignore_case=TRUE)) ~ "Peer", 
    str_detect(Var, regex("Boss_(LV|Avg)", ignore_case=TRUE)) ~ "Boss",
    str_detect(Var, regex("reputation", ignore_case=TRUE)) ~ "Reputation",
    str_detect(Var, regex("arena", ignore_case=TRUE)) ~ "Trait/Arena",
    str_detect(Var, regex("inf_avg", ignore_case=TRUE)) ~ "All Informants",
    str_detect(Var, regex("All_avg", ignore_case=TRUE)) ~ "All",
    TRUE ~ Var
  )) %>%
  mutate(Var = str_replace(Var, "_LV", "")) %>%
  mutate_all(~ ifelse(is.na(.), "◉", .))


reg_weights_table %>%
  kableExtra::kable(caption=sprintf("Associations Between <span style='color:#00838f;'>**%s**</span> Scores and Performance Criterion: Correlations and Regression", competency_full_name)) %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover")) %>%
  kableExtra::add_header_above(c(" " = 2, "Standardized Weights from Regression Models" = 3)) %>%
  kableExtra::pack_rows("TRI/LARI", start_row = 1, end_row = 6) %>% 
  kableExtra::pack_rows("Averaged", start_row = 7, end_row = 12) %>%
  kableExtra::footnote(
    general = sprintf("**N = %s**. Correlations ≥ |.03| are significant at p < .05. NS = Not Significant.", format(nrow(df_merged), big.mark=",")))


```


```{r garbage-collection, echo=FALSE, warning=FALSE}

invisible(gc())

```




